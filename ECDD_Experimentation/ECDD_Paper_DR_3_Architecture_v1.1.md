**Architecture Data-Flow -**

**Updated inference dataflow with explicit guardrails (production path, edge runtime):  
**Image or frame input (upload or camera) → file integrity checks and type allowlist (reject corrupt payloads, enforce max size, enforce supported formats) → decode to RGB in sRGB and enforce a single decoding library path (no mixed PIL/OpenCV behavior) → apply EXIF orientation and enforce a consistent gamma path (sRGB only; no implicit linearization) → color channel order validation (RGB not BGR) and dtype/range validation (uint8 0–255 or float 0–1, but one fixed policy) → face detection and alignment step with explicit confidence thresholds → routing guardrail: if no face detected or face confidence below threshold then output “Abstain/No-face/Low-confidence” and stop; if multiple faces then crop each face and run the model per-face and aggregate using max P-Fake (return worst-face heatmap) → image quality guardrails: minimum face crop size and minimum resolution; blur check; heavy compression indicator (e.g., very low JPEG quality proxy) → routing guardrail: if below minimum quality then output “Abstain/Low-quality” (or require a stricter fake threshold) → deterministic resize to 256 by 256 using a fixed interpolation kernel (must match training exactly) → normalize using the exact mean and std used in training → Student model on edge (Tiny-ML) → convolutional stack with small receptive field (patch-like behavior without explicit patch extraction) → patch-logit map over spatial grid (each cell is a local q by q evidence score) → pooling over patch logits to form a single image logit using the same pooling rule as training (attention-based, fixed weights at inference) (if found to be too complex or fucks with the quantization at construction), revert to top-k) → calibration layer on the image logit (temperature scaling or Platt scaling; parameters frozen at deploy time and versioned) → sigmoid to get calibrated P-Fake → decision logic: thresholds selected to satisfy product constraints (explicitly defined operating point, e.g., false positive target on real user images) with an explicit “Abstain/Low-confidence” band between thresholds → output: label (Real/Fake/Abstain), calibrated probability, confidence flag, patch heatmap, and guardrail reason codes if abstained.

**Updated training and export dataflow with explicit deployment-equivalence, calibration, and quantization handling:**  
Dataset intake (faces primarily; includes a small non-face/animal subset to test guardrails) → split policy definition: time-based split and source-based split (device/platform/compression) to reflect deployment; hold out an “in-the-wild” test set → preprocessing reproducibility checklist instantiated as code: single decode path, EXIF orientation handling, RGB channel order, deterministic resize kernel, fixed normalization, face crop policy (single-face or multi-face) exactly mirroring inference routing → smoke test harness: run a small fixed image set through both training preprocessing and deployed preprocessing and assert pixel-level agreement or bounded tolerances at each stage (decode, crop, resize, normalize) before any model training proceeds → teacher training (cloud preferred, laptop fallback): train LaDeDa teacher to output patch logits plus pooled image logit using the exact same input geometry and preprocessing as inference (deterministic 256 resize, same decode path, no random crop) and the same patch-grid geometry → teacher inference labeling: run teacher over training images/frames to generate patch-logit supervision targets at the same patch-grid resolution used by the student → student training (Tiny-LaDeDa): train student to match teacher with patch-level distillation loss (MSE acceptable; KL-on-logits with temperature preferred for stability) plus image-level binary cross entropy on labels, with the student pooling matched to the teacher pooling (attention pooling aligned by design) → robustness augmentation stage (only if deployment gap remains): apply adversarial transforms during training/evaluation that mirror deployment and attacks (re-encode JPEG at multiple qualities, resize down and up, blur, screenshot-like resampling, phone-recapture simulation if feasible) → calibration dataset construction: curate a deployment-calibration set from the target domain (local host pipeline outputs, device/platform mixes) separate from training data → calibration fitting: fit temperature scaling or Platt scaling on the calibration set; generate reliability curves and DET curves; choose and record the operating thresholds that meet risk constraints (for example, prioritize low false positives if false alarms are costly) → export: serialize student + pooling + calibration parameters + threshold policy as a single versioned bundle → convert to TFLite → quantize (dynamic range preferred; full-int8 if device requires) → post-quant calibration: re-fit temperature scaling and re-select thresholds on the same deployment-calibration set after quantization because quantization can shift score distributions → final evaluation protocol (pre-release gate): evaluate on “in-the-wild” sets, time-split sets, cross-device/cross-compression sets, and the adversarial transform suite; report operating-point metrics (FPR, FNR, TPR at fixed FPR), not only accuracy/AUC → release with locked preprocessing + locked guardrails + locked calibration + locked threshold policy, all versioned and reproducible.

Monitoring and maintenance dataflow (explicit, privacy-preserving, post-deploy loop):  
Deployed inference event → log summary features only (calibrated P-Fake, decision label, confidence/abstain reason codes, face-detector confidence, image resolution, compression proxy signals, model version, device/platform class if available) with no raw image storage unless explicit user consent → drift detection: monitor score distribution shifts and guardrail-trigger rates over time; detect sudden changes in percent flagged fake or percent abstained → trigger conditions: if drift or performance complaints exceed thresholds then initiate data review and collect a consented audit set → periodic recalibration: update temperature/Platt parameters and thresholds on fresh deployment-calibration data; if drift is severe, escalate to data augmentation or model upgrade tracks (patch-based pooling tweaks, frequency cues like NPR/DCT/FFT, diffusion-robust training like DRCT/DiffusionFake, universal CLIP-style detectors, or compression/distillation improvements for edge) → redeploy updated versioned bundle.

TO BE SPECIFIED LIST IS AS FOLLOWS -

- Pooling choice must be finalized and frozen (top-k vs attention). If attention is used, specify its exact architecture, parameterization, and how it behaves under quantization. If top-k is used, specify r, K computation, tie-breaking, and numerical stability.
- Explicitly freeze the entire pixel pipeline across browser, server, and edge. Specify the exact decoder/library, RGB channel ordering, sRGB/gamma handling, alpha handling (PNG/WebP), EXIF orientation handling, and whether any client-side resizing or re-encoding is allowed (and if so, exactly how).
- Define an end-to-end “equivalence contract” between training preprocessing and deployed preprocessing. This includes deterministic resize size, interpolation kernel, rounding rules, dtype/range, and normalization constants, plus explicit tolerances for pixel-level differences.
- Guardrails must be fully parameterized. Specify the face detector model, its confidence threshold, the alignment/crop policy, minimum face crop size, and the exact multi-face policy (process each face and aggregate by max, or pick largest face, etc.).
- Define the image quality gate precisely. Specify the blur metric (and threshold), minimum resolution thresholds, compression proxies (and thresholds), and the rule for what happens when quality is below threshold (abstain vs stricter threshold vs “needs review”).
- Define out-of-scope input policy. Explicitly state what happens for non-face images (animals, scenery, cartoons, CGI), and do not mix these into “accuracy” claims unless you define a separate scope and evaluation.
- Calibration method must be fixed and versioned. Specify whether you use temperature scaling, Platt scaling, or isotonic regression; how the calibration parameters are stored; and the rule that calibration parameters are frozen per deployed model version.
- Define the deployment-calibration set contract. Specify how it is sampled (device/platform/compression mix, time window), its minimum size, and how often it is refreshed.
- Define the operating-point contract. Specify the primary error budget (for example, “FPR on real user faces ≤ 5%”) and secondary constraints (for example, “FNR on known fake attempts ≤ 10%”), and how thresholds (including abstain band if used) are selected.
- Quantization policy must be fixed. Specify whether dynamic range or full-int8 is used, and require post-quant calibration and post-quant threshold selection as a mandatory gate.
- Define quantitative parity gates between float and TFLite models. Require tolerances at patch-logit level and pooled-logit level on a golden set, not only final labels.
- Make the evaluation battery executable and versioned. Specify exactly which test sets you will use (in-the-wild, time-split, cross-device, cross-compression), the transform suite (re-encode qualities, resize chains, blur severities, screenshot-like resampling), and the metrics reported at the operating point (FPR, FNR, TPR at fixed FPR, abstain rate, calibration error).
- Implement a CI smoke test that runs “upload to prediction” end-to-end and asserts intermediate invariants: face crop dimensions, resize kernel identity, normalization ranges, patch grid shape, pooling output parity, and calibration application.
- Define logging and privacy policy for monitoring. Specify exactly which scalars are logged (scores, decision, reason codes, face confidence, resolution, compression proxy), prohibit storing raw images by default, and if embeddings are ever logged specify aggregation/anonymization and retention.
- Define drift triggers and mandatory responses. Specify thresholds for score-distribution shifts, abstain-rate spikes, face-detection-failure spikes, and what action each trigger forces (recalibrate only vs augment data vs retrain vs method upgrade).
- Define a release gating policy. Specify the minimum acceptable performance at the operating point, the maximum allowed abstain rate, and the requirement that all preprocessing/calibration/quantization parity checks pass before any deployment.
- Define versioning and audit artifacts. Every deployed bundle must include: preprocessing version, model weights hash, pooling config, calibration params, threshold policy, guardrail thresholds, and the exact evaluation report used to approve release.
- Numerical determinism policy across environments. Specify seed control (if any), deterministic ops requirements, and acceptable nondeterminism (CPU vs GPU vs WebGL/Metal), including tolerances at each intermediate tensor.
- Exact patch grid contract. Specify the patch-logit map spatial dimensions, stride/effective receptive field, padding behavior, and the mapping from heatmap cells back to image coordinates (for UI overlays).
- Attention pooling determinism and quantization safety (if attention pooling is chosen). Specify whether attention uses softmax (and its numeric stability), whether it is computed in float32 even under int8 inference, and the allowed approximation error post-quant.
- Face detector lifecycle policy. Version pinning, calibration and thresholding for face confidence, failure modes (occlusions, profile faces), and a fallback behavior (for example, alternate face detector vs abstain).
- Client-side pipeline constraints for the local-host website. Specify whether any client-side resizing/compression occurs, whether the browser converts color profiles, and whether images are re-encoded before upload. If yes, those steps become part of the “locked preprocessing.”
- Throughput and latency budgets. Define target latency per image on the actual edge hardware, maximum memory footprint, maximum CPU usage, and concurrency behavior (single-thread vs multi-thread), because these affect model choice, input batching, and TFLite delegate configuration.
- TFLite runtime configuration contract. Specify delegate usage (XNNPACK, NNAPI, GPU delegate), thread count, float16 usage if any, and whether the same config is required across devices.
- Operational semantics for “Abstain.” Define what the UI does (block, ask for another image, escalate to manual review), whether abstentions count as errors in KPI reporting, and what target abstain rate is acceptable.
- Security and abuse controls. Define rate limiting, file size limits, decompression bomb protections, and denial-of-service protections, because these are common in image upload services and can also bias your monitoring if not controlled.
- Dataset governance for retraining. Define de-duplication, leakage checks (near-duplicate between train and test), label quality policy, and how you handle mixed generator families so your evaluation remains meaningful.
- Release, rollback, and canarying policy. Define how you roll out new model bundles, how you compare old vs new at the operating point, and when you automatically roll back if drift or error reports spike.
- Human feedback loop policy. Define what “manual review” means, how reviewed cases are stored (consent, retention), and how they are incorporated into recalibration vs retraining without contaminating held-out tests.

_For continuous improvement loop → edge devices fine-tune their respective student locally on new labeled data → send model updates → server aggregates updates (FedProx) → redistribute updated student weights to devices_. (TO BE UPDATED)

**Explanation -**

The updated inference dataflow is the exact, edge-runtime path from an input image to a decision, with guardrails explicitly placed before the model so obvious failure cases do not become “model errors.” First, the system enforces basic file integrity and a strict decoding path: it always decodes into RGB in sRGB, applies EXIF orientation, and avoids mixed-library behavior that silently changes color ordering or gamma. Next, it runs face detection and uses that as a routing decision: if no face is found or the face confidence is too low, the system abstains instead of guessing; if multiple faces exist, it processes each face crop separately and aggregates by taking the maximum fake probability, returning the worst-face heatmap. After that, quality guardrails check for conditions that are known to destabilize patch detectors (too-small crops, extreme blur, heavy compression), and either abstain or tighten the fake threshold under low-quality conditions. Only then does the image go through the deterministic 256-by-256 resize (fixed interpolation kernel) and the exact training normalization. The student model produces a patch-logit map, an attention-based pooling module produces a single image logit, a calibration layer (temperature or Platt scaling with frozen parameters) converts that logit into a calibrated probability, and the decision logic uses thresholds chosen for a specified operating point, optionally with a low-confidence “abstain band.” The output is not just Real or Fake, but Real or Fake or Abstain, plus the heatmap, the calibrated probability, and explicit reason codes if a guardrail is triggered.

The training and export dataflow is designed to eliminate the classic lab-to-production gap by making “deployment equivalence” a first-class gate before any training claims are believed. It starts by defining splits that reflect deployment risk: time-based splits (to surface drift), and source-based splits (device or platform and compression levels) so you can measure generalization, not memorization. Then the preprocessing reproducibility checklist is implemented as code, and a smoke-test harness asserts that the training preprocessing and deployed preprocessing are functionally identical at each stage: decode, EXIF handling, RGB ordering, face crop policy, deterministic resize kernel, and normalization. Only after this gate do you train the LaDeDa teacher to produce patch logits and an image logit using the exact same geometry and pooling behavior that inference will use. You run the teacher over training images to generate patch-logit supervision at the same patch grid resolution the student outputs, then train the student with a patch distillation loss (MSE is fine; KL-on-logits with temperature is often more stable) plus image-level binary cross-entropy. If the deployment gap persists after equivalence and calibration, you introduce robustness augmentation that simulates deployment conditions (recompression, resize chains, blur, screenshot-like resampling) and re-evaluate. Separately, you create a deployment-calibration set that matches the real input stream, fit temperature or Platt calibration on that set, and select thresholds based on an explicitly stated risk constraint (for example, a strict false positive budget). You then export a single versioned bundle that includes the model, pooling, calibration parameters, and threshold policy, convert to TFLite, quantize, and crucially re-do calibration and threshold selection post-quantization because quantization often shifts score distributions. Final release is gated on evaluation that mirrors deployment: in-the-wild sets, time splits, cross-device and cross-compression cuts, and the transform suite, reporting operating-point metrics (false positives and false negatives at chosen thresholds) rather than only AUC.

The monitoring and maintenance dataflow is the post-deployment control loop that keeps performance stable as inputs and attacks evolve, without relying on invasive telemetry. Each inference event logs only privacy-preserving summary signals: calibrated probability, final decision, abstain or guardrail reason codes, face detector confidence, basic file and resolution attributes, compression proxy signals, and the model version. You then track distributions over time: score histograms, the rate of “fake” flags, and the rate of abstentions, because a sudden shift in any of these is often the earliest indicator of drift or a pipeline change upstream. You define trigger conditions such as a sharp increase in abstain rate (suggesting new low-quality inputs) or a shift in score distribution (suggesting new content or a new generator), and when triggers fire you investigate using a consented audit set rather than silently storing images. The primary corrective action is periodic recalibration: refit temperature or Platt parameters and re-select thresholds on fresh deployment-calibration data. If recalibration is insufficient, you escalate to data augmentation or retraining, and if shift is structural you move to method upgrades that improve generalization (stronger patch pooling, frequency-based cues like NPR or DCT/FFT features, diffusion-robust training, universal CLIP-style detectors), while keeping the edge constraints and latency budgets explicit. This loop ensures you are not chasing a single static accuracy number, but maintaining a stable operating point over time with a clear, auditable response plan when reality changes.

**FAQ: Terms in the Deepfake Detector Architecture and Specification**

1.  **What is EXIF orientation?**  
    Many photos (especially from phones) store the “correct rotation” as metadata instead of rotating the pixel array itself. That metadata is called EXIF orientation. If you ignore it, a portrait image might be decoded sideways or upside down, which changes the face crop, the resize result, and ultimately the model score. In your pipeline, “apply EXIF orientation” means you always rotate or flip the pixels to the intended upright orientation before any face detection or resizing.
2.  **What do we mean by “mixed library behavior,” “color ordering,” and “gamma”?**  
    Mixed library behavior means different image libraries decode the same file into slightly different pixel arrays. For example, one path might use Pillow (PIL) and another might use OpenCV; they can differ in how they handle color profiles, EXIF rotation, alpha channels, and even rounding. If your training used one decode path and production uses another, you can get a real deployment gap.

Color ordering refers to whether the pixel channels are interpreted as RGB (red-green-blue) or BGR (blue-green-red). OpenCV commonly uses BGR by default, while most deep learning pipelines assume RGB. If you accidentally feed BGR as if it were RGB, the model sees a different image and performance collapses.

Gamma refers to how pixel values relate to actual brightness. Most images are in sRGB, which is a nonlinear encoding designed for display. Some pipelines implicitly convert to a linear brightness space or apply color management, while others do not. “Consistent gamma path” means you pick one rule (typically: decode as sRGB and do not implicitly linearize) and enforce it everywhere so training and inference match.

1.  **What do we mean by a “fixed interpolation kernel” and “training normalization”?  
    **When you resize an image to 256 by 256, you must choose an interpolation method (the kernel), such as nearest-neighbor, bilinear, bicubic, or Lanczos. Different kernels produce different pixel values, especially around edges and textures, which are exactly where deepfake artifacts often live. “Fixed interpolation kernel” means you hard-code the exact resize method and parameters and ensure training and deployment use the same one.

Training normalization means the exact transformation you apply to pixel values before feeding the model, usually converting to float and applying mean and standard deviation normalization per channel. For example, you might scale pixels to 0–1 and then subtract a mean and divide by a standard deviation for each channel. If training used one mean/std and inference uses another (or uses 0–255 instead of 0–1), the model receives a shifted distribution and predictions become unreliable.

1.  **What is a patch-logit and what is a patch-logit map?**  
    A logit is the raw score produced by the model before applying a sigmoid. For binary classification, a higher logit means “more fake,” lower means “more real,” before converting to probability. A patch-logit is the same concept, but computed for a local region (a “patch”) rather than the whole image.

A patch-logit map is the grid of these patch logits over the image. Imagine the model outputs a 2D grid where each cell corresponds to a local region of the face, and each cell contains a logit indicating how fake that region looks. This map is useful both for pooling (combining patch evidence into one image score) and for visualization as a heatmap.

1.  **What are temperature scaling and Platt scaling?**  
    Both are calibration methods, meaning they adjust model scores so that predicted probabilities better match reality. A model can rank examples well (high AUC) but be overconfident or underconfident, which breaks thresholding in production.

Temperature scaling takes the image logit and divides it by a learned positive scalar temperature T before applying sigmoid. It does not change ranking much, but it can fix overconfidence and improve probability reliability.

Platt scaling fits a simple logistic regression on the model’s raw score: it learns two parameters (a and b) so that the calibrated score is sigmoid(a times logit plus b). It is slightly more flexible than temperature scaling and can shift and rescale scores.

In your system, calibration parameters are trained once on a deployment-like calibration set and then frozen and versioned with the model.

1.  **What are time-based splits and what do we mean by “surface drift”?**  
    A time-based split means you split data by when it was collected, not randomly. You train on earlier data and test on later data. This simulates real deployment, where the future is not identical to the past.

Surface drift means the input distribution changes over time in visible ways: different phone cameras, different compression pipelines, different social media processing, different lighting styles, or simply new fake generators. Even if the task is the same, these surface-level changes can shift the model’s score distributions and break a previously good threshold. Time-based splits help you detect this risk before deployment.

1.  **What is Platt calibration and what is “KL-on-logits”?**  
    Platt calibration is the same as Platt scaling: fitting a logistic regression layer on top of model scores to turn them into calibrated probabilities. It is a post-hoc step, not retraining the whole network.

KL-on-logits refers to a distillation loss used when training a student model from a teacher. The teacher produces logits; you convert them into a soft probability distribution using a softmax or sigmoid with a temperature, and you train the student to match that soft distribution using KL divergence (a measure of how different two distributions are). This often works better than plain MSE on logits because it focuses on matching relative confidence patterns rather than raw numeric values. In your case, “KL-on-logits with temperature” is a common distillation choice when you want the student to inherit the teacher’s decision behavior smoothly.