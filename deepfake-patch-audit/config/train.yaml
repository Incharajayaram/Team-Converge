# Training hyperparameters

training:
  epochs: 50
  batch_size: 32
  learning_rate: 0.001
  weight_decay: 1e-4

  # Optimizer
  optimizer: "adam"
  momentum: 0.9

  # Learning rate schedule
  scheduler: "cosine"
  warmup_epochs: 5
  min_lr: 1e-6

  # Checkpointing
  save_interval: 5  # epochs
  save_best: true

  # Logging
  log_interval: 100  # iterations
  val_interval: 5  # epochs

  # Device
  device: "cuda"
  mixed_precision: false

  # Seed
  seed: 42

# Knowledge distillation (student training)
distillation:
  temperature: 4.0
  alpha_distill: 0.05  # weight of patch MSE loss (START SMALL, increase after confirming learning)
  alpha_task: 0.95     # weight of image BCE loss (START HIGH to ensure task learning)
  use_attention_transfer: false
  # Recommended progression:
  # 1. alpha_distill=0.01, alpha_task=0.99  (confirm BCE loss works first)
  # 2. alpha_distill=0.05, alpha_task=0.95  (introduce light distillation)
  # 3. alpha_distill=0.1, alpha_task=0.9    (increase distillation signal)
  # 4. alpha_distill=0.5, alpha_task=0.5    (balanced distillation)
