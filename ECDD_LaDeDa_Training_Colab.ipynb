{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECDD LaDeDa Deepfake Detection Training\n",
        "\n",
        "**Free GPU Training on Kaggle**\n",
        "\n",
        "This notebook trains a LaDeDa-style ResNet50 for deepfake detection.\n",
        "\n",
        "## Setup Instructions:\n",
        "1. Upload your dataset to Kaggle Datasets (see cell below)\n",
        "2. Enable GPU: Settings \u2192 Accelerator \u2192 GPU P100\n",
        "3. Run all cells\n",
        "\n",
        "**Expected Training Time**: ~30-45 min for 15 epochs on P100\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install any missing packages\n",
        "# pip install -q torchvision pillow tqdm scikit-learn  # Uncomment with ! prefix in Jupyter\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import io\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from PIL import Image, ImageOps, ImageEnhance\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "# Check GPU\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset Setup\n",
        "\n",
        "### Option A: Upload to Kaggle Datasets (Recommended)\n",
        "1. Go to kaggle.com \u2192 Your Work \u2192 Datasets \u2192 New Dataset\n",
        "2. Upload your training data with structure:\n",
        "```\n",
        "ecdd-training-data/\n",
        "\u251c\u2500\u2500 train/\n",
        "\u2502   \u251c\u2500\u2500 real/\n",
        "\u2502   \u2502   \u2514\u2500\u2500 *.jpg\n",
        "\u2502   \u2514\u2500\u2500 fake/\n",
        "\u2502       \u2514\u2500\u2500 *.jpg\n",
        "\u251c\u2500\u2500 val/\n",
        "\u2502   \u251c\u2500\u2500 real/\n",
        "\u2502   \u2514\u2500\u2500 fake/\n",
        "\u2514\u2500\u2500 test/\n",
        "    \u251c\u2500\u2500 real/\n",
        "    \u2514\u2500\u2500 fake/\n",
        "```\n",
        "3. Add dataset to this notebook: \"+ Add Data\" \u2192 Your Datasets\n",
        "\n",
        "### Option B: Use Existing Public Dataset\n",
        "You can also use Celeb-DF or FaceForensics++ from Kaggle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== CONFIGURE YOUR DATASET PATH HERE ==========\n",
        "# If you uploaded your own dataset:\n",
        "DATA_PATH = \"/kaggle/input/ecdd-training-data\"\n",
        "\n",
        "# Alternative: Use a public dataset (uncomment one):\n",
        "# DATA_PATH = \"/kaggle/input/celeb-df-v2\"\n",
        "# DATA_PATH = \"/kaggle/input/faceforensics\"\n",
        "\n",
        "# Check if path exists\n",
        "if os.path.exists(DATA_PATH):\n",
        "    print(f\"\u2705 Dataset found at: {DATA_PATH}\")\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        split_path = os.path.join(DATA_PATH, split)\n",
        "        if os.path.exists(split_path):\n",
        "            real_count = len(list(Path(split_path).glob('real/*.jpg')))\n",
        "            fake_count = len(list(Path(split_path).glob('fake/*.jpg')))\n",
        "            print(f\"   {split}: {real_count} real, {fake_count} fake\")\n",
        "else:\n",
        "    print(f\"\u274c Dataset not found at: {DATA_PATH}\")\n",
        "    print(\"Please upload your dataset or modify DATA_PATH\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Architecture: LaDeDa ResNet50\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionPooling(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention-based pooling over patch logits.\n",
        "    Learns to weight patches based on their \"importance\" for the final decision.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels: int = 2048, hidden_dim: int = 512):\n",
        "        super().__init__()\n",
        "        self.attention_fc = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_dim, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(hidden_dim, 1, kernel_size=1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, features: torch.Tensor, patch_logits: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Compute attention scores\n",
        "        attention_scores = self.attention_fc(features)  # (B, 1, H, W)\n",
        "        \n",
        "        # Flatten spatial dimensions for softmax\n",
        "        B, _, H, W = attention_scores.shape\n",
        "        attention_flat = attention_scores.view(B, -1)  # (B, H*W)\n",
        "        \n",
        "        # Apply softmax for normalized weights\n",
        "        attention_weights_flat = F.softmax(attention_flat, dim=1)  # (B, H*W)\n",
        "        attention_weights = attention_weights_flat.view(B, 1, H, W)  # (B, 1, H, W)\n",
        "        \n",
        "        # Weighted sum of patch logits\n",
        "        patch_logits_flat = patch_logits.view(B, -1)  # (B, H*W)\n",
        "        pooled_logit = (patch_logits_flat * attention_weights_flat).sum(dim=1, keepdim=True)  # (B, 1)\n",
        "        \n",
        "        return pooled_logit, attention_weights\n",
        "\n",
        "\n",
        "class LaDeDaResNet50(nn.Module):\n",
        "    \"\"\"\n",
        "    LaDeDa-style ResNet50 for patch-based deepfake detection.\n",
        "    \n",
        "    Key modifications:\n",
        "    - Replace 7x7 conv with 3x3 (smaller receptive field)\n",
        "    - Remove maxpool\n",
        "    - Patch-level classification with attention pooling\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, \n",
        "                 pretrained: bool = True,\n",
        "                 freeze_layers: Optional[list] = None,\n",
        "                 num_classes: int = 1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Load base ResNet50\n",
        "        if pretrained:\n",
        "            base_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
        "        else:\n",
        "            base_model = resnet50(weights=None)\n",
        "        \n",
        "        # MODIFICATION 1: Replace conv1 (7x7 -> 3x3)\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        \n",
        "        if pretrained:\n",
        "            with torch.no_grad():\n",
        "                original_weight = base_model.conv1.weight.data\n",
        "                center = original_weight[:, :, 2:5, 2:5]\n",
        "                self.conv1.weight.data = center\n",
        "        \n",
        "        self.bn1 = base_model.bn1\n",
        "        self.relu = base_model.relu\n",
        "        # MODIFICATION 2: NO maxpool (removed)\n",
        "        \n",
        "        self.layer1 = base_model.layer1\n",
        "        self.layer2 = base_model.layer2\n",
        "        self.layer3 = base_model.layer3\n",
        "        self.layer4 = base_model.layer4\n",
        "        \n",
        "        # MODIFICATION 3: Patch classifier\n",
        "        self.patch_classifier = nn.Conv2d(2048, num_classes, kernel_size=1)\n",
        "        \n",
        "        # MODIFICATION 4: Attention pooling\n",
        "        self.attention_pool = AttentionPooling(in_channels=2048)\n",
        "        \n",
        "        # Freeze layers\n",
        "        self.freeze_layers = freeze_layers or []\n",
        "        self._freeze_layers()\n",
        "        \n",
        "    def _freeze_layers(self):\n",
        "        freeze_map = {\n",
        "            'conv1': [self.conv1, self.bn1],\n",
        "            'layer1': [self.layer1],\n",
        "            'layer2': [self.layer2],\n",
        "            'layer3': [self.layer3],\n",
        "            'layer4': [self.layer4],\n",
        "        }\n",
        "        \n",
        "        for layer_name in self.freeze_layers:\n",
        "            if layer_name in freeze_map:\n",
        "                for module in freeze_map[layer_name]:\n",
        "                    for param in module.parameters():\n",
        "                        param.requires_grad = False\n",
        "                print(f\"Frozen: {layer_name}\")\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        # NO maxpool\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        \n",
        "        features = x\n",
        "        patch_logits = self.patch_classifier(features)\n",
        "        pooled_logit, attention_map = self.attention_pool(features, patch_logits)\n",
        "        \n",
        "        return pooled_logit, patch_logits, attention_map\n",
        "\n",
        "\n",
        "def create_ladeda_model(pretrained=True, freeze_layers=None):\n",
        "    \"\"\"Create LaDeDa model.\"\"\"\n",
        "    return LaDeDaResNet50(pretrained=pretrained, freeze_layers=freeze_layers)\n",
        "\n",
        "\n",
        "# Test model\n",
        "print(\"Testing model architecture...\")\n",
        "test_model = create_ladeda_model(pretrained=True, freeze_layers=['conv1', 'layer1'])\n",
        "test_model.eval()\n",
        "with torch.no_grad():\n",
        "    x = torch.randn(2, 3, 256, 256)\n",
        "    pooled, patches, attention = test_model(x)\n",
        "print(f\"\u2705 Model OK - Pooled: {pooled.shape}, Patches: {patches.shape}, Attention: {attention.shape}\")\n",
        "\n",
        "total = sum(p.numel() for p in test_model.parameters())\n",
        "trainable = sum(p.numel() for p in test_model.parameters() if p.requires_grad)\n",
        "print(f\"Parameters: Total={total:,}, Trainable={trainable:,}\")\n",
        "del test_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Dataset & Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ECDD-locked preprocessing constants\n",
        "TARGET_SIZE = (256, 256)\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "class DeepfakeDataset(Dataset):\n",
        "    \"\"\"Dataset for deepfake detection with ECDD-compliant preprocessing.\"\"\"\n",
        "    \n",
        "    def __init__(self, data_dir: str, split: str = \"train\", augment: bool = True):\n",
        "        self.data_dir = Path(data_dir) / split\n",
        "        self.augment = augment and (split == \"train\")\n",
        "        \n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        \n",
        "        # Load images\n",
        "        for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
        "            for f in (self.data_dir / \"real\").glob(ext):\n",
        "                self.images.append(f)\n",
        "                self.labels.append(0)\n",
        "            for f in (self.data_dir / \"fake\").glob(ext):\n",
        "                self.images.append(f)\n",
        "                self.labels.append(1)\n",
        "        \n",
        "        print(f\"Loaded {split}: {len(self.images)} images (Real: {self.labels.count(0)}, Fake: {self.labels.count(1)})\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def _augment(self, img):\n",
        "        \"\"\"Apply training augmentations.\"\"\"\n",
        "        # Random horizontal flip\n",
        "        if random.random() > 0.5:\n",
        "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        \n",
        "        # Random JPEG compression (simulate social media)\n",
        "        if random.random() > 0.5:\n",
        "            quality = random.randint(50, 95)\n",
        "            buffer = io.BytesIO()\n",
        "            img.save(buffer, format='JPEG', quality=quality)\n",
        "            buffer.seek(0)\n",
        "            img = Image.open(buffer)\n",
        "            img.load()\n",
        "        \n",
        "        # Random brightness\n",
        "        if random.random() > 0.7:\n",
        "            factor = random.uniform(0.9, 1.1)\n",
        "            img = ImageEnhance.Brightness(img).enhance(factor)\n",
        "        \n",
        "        return img\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        # Load and preprocess (ECDD-locked)\n",
        "        img = Image.open(img_path)\n",
        "        img = ImageOps.exif_transpose(img)  # Fix EXIF orientation\n",
        "        img = img.convert('RGB')\n",
        "        \n",
        "        if self.augment:\n",
        "            img = self._augment(img)\n",
        "        \n",
        "        # Resize with Lanczos (ECDD-locked)\n",
        "        img = img.resize(TARGET_SIZE, Image.Resampling.LANCZOS)\n",
        "        \n",
        "        # To tensor and normalize\n",
        "        img_array = np.array(img).astype(np.float32) / 255.0\n",
        "        img_array = (img_array - IMAGENET_MEAN) / IMAGENET_STD\n",
        "        img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float()\n",
        "        \n",
        "        return img_tensor, torch.tensor(label, dtype=torch.float32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(outputs, labels, threshold=0.5):\n",
        "    \"\"\"Compute accuracy, precision, recall, F1.\"\"\"\n",
        "    probs = torch.sigmoid(outputs).cpu().numpy()\n",
        "    preds = (probs > threshold).astype(int)\n",
        "    labels = labels.cpu().numpy().astype(int)\n",
        "    \n",
        "    accuracy = (preds == labels).mean()\n",
        "    \n",
        "    tp = ((preds == 1) & (labels == 1)).sum()\n",
        "    fp = ((preds == 1) & (labels == 0)).sum()\n",
        "    fn = ((preds == 0) & (labels == 1)).sum()\n",
        "    \n",
        "    precision = tp / (tp + fp + 1e-8)\n",
        "    recall = tp / (tp + fn + 1e-8)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "    \n",
        "    return {\n",
        "        'accuracy': float(accuracy),\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1': float(f1)\n",
        "    }\n",
        "\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, scaler, device):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_outputs = []\n",
        "    all_labels = []\n",
        "    \n",
        "    pbar = tqdm(dataloader, desc=\"Training\")\n",
        "    for images, labels in pbar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        with autocast():\n",
        "            pooled_logit, _, _ = model(images)\n",
        "            loss = criterion(pooled_logit.squeeze(), labels)\n",
        "        \n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        all_outputs.append(pooled_logit.squeeze().detach())\n",
        "        all_labels.append(labels)\n",
        "        \n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "    \n",
        "    all_outputs = torch.cat(all_outputs)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "    metrics = compute_metrics(all_outputs, all_labels)\n",
        "    metrics['loss'] = total_loss / len(dataloader)\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    \"\"\"Validate the model.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_outputs = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Validation\"):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            pooled_logit, _, _ = model(images)\n",
        "            loss = criterion(pooled_logit.squeeze(), labels)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            all_outputs.append(pooled_logit.squeeze())\n",
        "            all_labels.append(labels)\n",
        "    \n",
        "    all_outputs = torch.cat(all_outputs)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "    metrics = compute_metrics(all_outputs, all_labels)\n",
        "    metrics['loss'] = total_loss / len(dataloader)\n",
        "    \n",
        "    return metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Configuration\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== TRAINING CONFIG ==========\n",
        "CONFIG = {\n",
        "    'name': 'ladeda_deepfake',\n",
        "    'epochs': 15,\n",
        "    'batch_size': 16,  # Reduce to 8 if OOM\n",
        "    'lr': 1e-4,\n",
        "    'weight_decay': 1e-4,\n",
        "    'freeze_layers': ['conv1', 'layer1'],  # Freeze early layers\n",
        "}\n",
        "\n",
        "OUTPUT_DIR = \"/kaggle/working/checkpoints\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Run Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "train_dataset = DeepfakeDataset(DATA_PATH, split=\"train\", augment=True)\n",
        "val_dataset = DeepfakeDataset(DATA_PATH, split=\"val\", augment=False)\n",
        "test_dataset = DeepfakeDataset(DATA_PATH, split=\"test\", augment=False)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], \n",
        "                          shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], \n",
        "                        shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], \n",
        "                         shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"\\nTrain batches: {len(train_loader)}, Val batches: {len(val_loader)}, Test batches: {len(test_loader)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = create_ladeda_model(pretrained=True, freeze_layers=CONFIG['freeze_layers'])\n",
        "model = model.to(device)\n",
        "\n",
        "# Training setup\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=CONFIG['lr'],\n",
        "    weight_decay=CONFIG['weight_decay']\n",
        ")\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'])\n",
        "scaler = GradScaler()\n",
        "\n",
        "print(f\"Model loaded on {device}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "best_val_f1 = 0\n",
        "history = {'train': [], 'val': []}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Starting Training\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in range(CONFIG['epochs']):\n",
        "    print(f\"\\nEpoch {epoch+1}/{CONFIG['epochs']}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    train_metrics = train_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
        "    val_metrics = validate(model, val_loader, criterion, device)\n",
        "    \n",
        "    scheduler.step()\n",
        "    \n",
        "    history['train'].append(train_metrics)\n",
        "    history['val'].append(val_metrics)\n",
        "    \n",
        "    print(f\"Train - Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.4f}, F1: {train_metrics['f1']:.4f}\")\n",
        "    print(f\"Val   - Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1']:.4f}\")\n",
        "    \n",
        "    # Save best model\n",
        "    if val_metrics['f1'] > best_val_f1:\n",
        "        best_val_f1 = val_metrics['f1']\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_metrics': val_metrics,\n",
        "            'config': CONFIG\n",
        "        }, f\"{OUTPUT_DIR}/best_model.pth\")\n",
        "        print(f\"  \u2192 Saved best model (Val F1: {best_val_f1:.4f})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Final Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Final Test Evaluation\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load best model\n",
        "checkpoint = torch.load(f\"{OUTPUT_DIR}/best_model.pth\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "test_metrics = validate(model, test_loader, criterion, device)\n",
        "print(f\"Test - Loss: {test_metrics['loss']:.4f}, Acc: {test_metrics['accuracy']:.4f}\")\n",
        "print(f\"       Precision: {test_metrics['precision']:.4f}, Recall: {test_metrics['recall']:.4f}, F1: {test_metrics['f1']:.4f}\")\n",
        "\n",
        "# Save results\n",
        "results = {\n",
        "    'config': CONFIG,\n",
        "    'best_val_f1': best_val_f1,\n",
        "    'test_metrics': test_metrics,\n",
        "    'history': history,\n",
        "    'timestamp': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "with open(f\"{OUTPUT_DIR}/results.json\", 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\n\u2705 Training complete! Results saved to {OUTPUT_DIR}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Download Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your trained model is saved at:\n",
        "print(\"\ud83d\udce6 Download your trained model from:\")\n",
        "print(f\"   {OUTPUT_DIR}/best_model.pth\")\n",
        "print(f\"   {OUTPUT_DIR}/results.json\")\n",
        "\n",
        "# List files in output\n",
        "print(\"\\n\ud83d\udcc1 Output files:\")\n",
        "for f in Path(OUTPUT_DIR).glob(\"*\"):\n",
        "    size = f.stat().st_size / 1024  # KB\n",
        "    print(f\"   {f.name}: {size:.1f} KB\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Training History Visualization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Loss\n",
        "axes[0].plot([m['loss'] for m in history['train']], label='Train')\n",
        "axes[0].plot([m['loss'] for m in history['val']], label='Val')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training & Validation Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Accuracy\n",
        "axes[1].plot([m['accuracy'] for m in history['train']], label='Train')\n",
        "axes[1].plot([m['accuracy'] for m in history['val']], label='Val')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('Training & Validation Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "# F1 Score\n",
        "axes[2].plot([m['f1'] for m in history['train']], label='Train')\n",
        "axes[2].plot([m['f1'] for m in history['val']], label='Val')\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].set_ylabel('F1 Score')\n",
        "axes[2].set_title('Training & Validation F1')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUTPUT_DIR}/training_history.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\ud83d\udcc8 Training history saved to {OUTPUT_DIR}/training_history.png\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}